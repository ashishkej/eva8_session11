# EVA8 Session11 Assignment

## Assignment
- Add these features to BERT training:
    - Collect your own data (cannot be Shakespeare or any single file downloaded from the internet. Your sources should come from multiple URLs (basically copy paste 1000s of times)
      - Added text from different books present in [Project Gutenberg](https://www.gutenberg.org/)
    - noisy word prediction (swap any word 15% of times from a sentence with any other random word, and then predict the correct word):
        - Share a sample from your own dataset
        - Share the training log (Epochs/x = 10 logs)
        -  Share 10 examples of input-output
- Implement sparse attention on your own in the GPT code that we wrote.     
    - Train on the data that you collected above:
    - Copy and paste the code here for the sparse attention that you wrote
        - share the training log (Epochs/x = 10 logs)
        - Share 10 examples of output

# Submission
[ BERT Notebook](https://github.com/ashishkej/eva8_session9/blob/main/EVA8_Session9_Assignment.ipynb)

[GPT Notebook](https://github.com/ashishkej/eva8-pytorch-models)




## References

* 

